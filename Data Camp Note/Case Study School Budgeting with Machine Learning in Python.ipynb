{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as categories\n",
    "sample_data.label.head()\n",
    "sample_data.label = sample_data.label.asdtype('category')\n",
    "# Dummy variable encoding \n",
    "dummies = pd.get_dummies(sample_df[['label']], prefix_sep = '_'ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lambda functions \n",
    "square = lambda x: x*x \n",
    "square(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function \n",
    "categorize_label = lambda x: x.astype('category')\n",
    "# apply function \n",
    "sample_df.label = sample_df[['label']].apply(categorize_label, axis = 0)\n",
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing log loss with Numpy \n",
    "# logloss.py\n",
    "import numpy as np\n",
    "def compute_log_loss(predicted, actual, eps = 1e-14):\n",
    "    predicted = np.clip(predicted, eps, 1 = eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted)\n",
    "                       + (1 - actual)\n",
    "                       * np.log(1 - predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print log loss for 1st case\n",
    "correct_confident_loss = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident_loss)) \n",
    "\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident_loss = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident_loss = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "wrong_confident_loss = compute_log_loss(wrong_confident, actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident_loss)) \n",
    "\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels_loss = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels_loss)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the multi-class datasets\n",
    "* StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the data\n",
    "data_to_train = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "labels_to_use = pd.get_dummies(df[LABELS])\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "                                    data_to_train,\n",
    "                                    labels_to_use, \n",
    "                                    size = 0.2, seed = 123)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making predictions \n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "hold_out = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "predictions = clf.predict_proba(hold_out)\n",
    "prediction_df = pd.DataFrame(columns = pd.get_dummies(df[LABELS]),prefix_sep = '__').columns,\n",
    "                             index = hold_out.index,\n",
    "                             data = predictions)\n",
    "score = score_submission(pred_path = 'predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A very brief introduction to NLP\n",
    "* Tokenization: Spliting a string into segments \n",
    "    \n",
    "### Bag of words representation\n",
    "* Count the number of times a particular token appears \n",
    "* This approach discards information about word order\n",
    "\n",
    "### n-grams\n",
    "\n",
    "### Representing text numerically \n",
    "* Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "TOKEN_BASIC = '\\\\\\\\S+(?=\\\\\\\\s+)'\n",
    "df.Program_Description.fillna('', inplace = True)\n",
    "vec_basic = CountVectorizer(token_pattern = TOKEN_BASIC)\n",
    "vec_basic.fit(df.Program_Description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipline, feature & text processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate simple pipline with one step\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "pl = Pipeline([\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "])\n",
    "\n",
    "# Train and test with sample numeric data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    sample_df[['numeric']],\n",
    "                                    pd.get_dummies( sample_df[['label']]), \n",
    "                                    random_state = 2)\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "\n",
    "# Adding more steps to the pipline \n",
    "from sklearn.preprocessing import Imputer\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    sample_df[['numeric', 'with_missing']],\n",
    "                                    pd.get_dummies( sample_df[['label']]), \n",
    "                                    random_state = 2)\n",
    "# Processing numeric features with missing data \n",
    "pl = Pipeline([\n",
    "    ('imp', Imputer()),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "])\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features and feature unions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import CountVectorizer \n",
    "from sklearn.feature_extraction.text import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    sample_df[['text']],\n",
    "                                    pd.get_dummies( sample_df[['label']]), \n",
    "                                    random_state = 2)\n",
    "pl = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "])\n",
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing mutiple dtypes\n",
    "* FunctionTransformer: \n",
    "    Turns a Python function into a object that sklearn pipline would understand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    sample_df[['numeric', 'with_missing', 'text']],\n",
    "                                    pd.get_dummies( sample_df[['label']]), \n",
    "                                    random_state = 2)\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], \n",
    "                                    validate = False)\n",
    "get_numeric_data = FeatureUnion(lambda x: x[['numeric', 'with_missing']],\n",
    "                                validate = False)\n",
    "# Fit and transform the text data: just_text_data\n",
    "just_text_data = get_text_data.fit_transform(sample_df)\n",
    "\n",
    "# Fit and transform the numeric data: just_numeric_data\n",
    "just_numeric_data = get_numeric_data.fit_transform(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion \n",
    "union = FeatureUnion([\n",
    "    ('numeric', numeric_pipeline),\n",
    "    ('text', text_pipepline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together \n",
    "numeric_pipeline = Pipeline([\n",
    "    ('selector', get_numeric_data),\n",
    "    ('imputer', Imputer())\n",
    "    ])\n",
    "\n",
    "text_pipeline = Pipline([\n",
    "    ('selector', get_text_data),\n",
    "    ('vectorizer', CountVctorizer())\n",
    "    ])\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('numeric', numeric_pipline),\n",
    "        ('text', text_pipline)\n",
    "    ])),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a classification model \n",
    "LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type', \n",
    "          'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "len(NON_LABELS) - len(NUMERIC_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pipeline with the main dataset\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "df = pd.read_csv('TrainingSetSample.csv', index_col = 0)\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "                                   df[NON_LABELS], dummy_labels, 0.2)\n",
    "get_text_data = FunctionTransformer(combine_text_columns,\n",
    "                                    validate = False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS],\n",
    "                                      validate = False)\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('imputer', Imputer())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('vectorizer', CountVctorizer())\n",
    "            ]))\n",
    "        ])\n",
    "    ),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "])\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily try new models using pipeline \n",
    "from sklearn.emsemble import RandomForestClassifier\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion([\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('imputer', Imputer())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('vectorizer', CountVctorizer())\n",
    "            ]))\n",
    "        ])\n",
    "    ),\n",
    "    ('clf', OneVsRest(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams and tokenization\n",
    "vec = CountVectorizer(token_pattern = TOKEN_ALPHANUMERIC, \n",
    "                      ngram_range = (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer: text_features\n",
    "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit text_features to the text vector\n",
    "text_features.fit(text_vector)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(text_features.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import other preprocessing modules\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement interaction modeling in scikit-learn\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),  \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree = 2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash trick \n",
    "* Useful on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer \n",
    "vec = HashingVectorizer(norm = None, \n",
    "                        non_negative = True,\n",
    "                        token_pattern = TOKENS_ALPHANUMERIC,\n",
    "                        ngram_range = (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Random Forest \n",
    "https://www.youtube.com/watch?v=loNcrMjYh64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
