{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree for classification \n",
    "\n",
    "### Chapter 1: Classification and Regression Tree (CART)\n",
    "* Sequence of if-else questions about individual features \n",
    "* Objective: infer class labels \n",
    "* Able to capture non-linear relationships between features and labels \n",
    "* Don't need standardirization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification-tree in scikit-learn \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklear.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 1)\n",
    "dt = DecisionTreeClassifier(max_depth = 2, random_state = 1)\n",
    "dt.fit(X_train, y_train)\n",
    "y_predict = dt.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks of a decision tree\n",
    "* Decision tree: data structure consisting of a hierarchy nodes\n",
    "* Node: question or prediction \n",
    "\n",
    "### Information gain (IG)\n",
    "\n",
    "Criteria to measure the impurity of a node I:\n",
    "* gini index,\n",
    "* entropy \n",
    "\n",
    "### Classification tree learning \n",
    "* Nodes are grown recursively \n",
    "* At each nodes, split the data based on: \n",
    "If IG = 0, declear the node a leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion = 'gini', random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree for Regression\n",
    "\n",
    "#### Auto-mpg Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 3)\n",
    "dt = DecisionTreeRegressor(max_depth = 4, \n",
    "                           min_samples_leaf = 0.1,\n",
    "                           random_state = 3)\n",
    "dt.fit(X_train, y_train)\n",
    "y_predict = dt.predict(X_test)\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "rmse_dt = mse_dt ** (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization Error \n",
    "* Low bias: Accurate\n",
    "* Low variance: Precise \n",
    "* As the complexity of fÌ‚  increases, the bias term decreases while the variance term increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold CV in sklearn on the auto Dataset\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.odel_selection import cross_val_score\n",
    "SEED = 123\n",
    "# 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = SEED)\n",
    "dt = DecisionTreeRegressor(max_depth = 4, \n",
    "                           min_sample_leaf = 0.14,\n",
    "                           random_state = SEED)\n",
    "MSE_CV = - cross_val_score(dt, X_train, y_train, cv = 10, \n",
    "                           scoring = 'neg_mean_squared error', \n",
    "                           n_jobs = -1)\n",
    "dt.fit(X_train, y_train)\n",
    "y_predict_train = dt.predict(X_train)\n",
    "y_predict_test = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning \n",
    "* Train different models on the same dataset\n",
    "* Let each model make its predictions \n",
    "* Meta-model: aggregates predictions of individudal models \n",
    "* Final prediction: more robust and less prone to errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier in sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClaasifier as KNN\n",
    "from sklearn.ensamble import VotingClassifier\n",
    "\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = SEED)\n",
    "lr = LogisticRegression(random_state = SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeRegressor(random_state = SEED)\n",
    "classifier = [('LogisticRegression', lr), \n",
    "              ('K Nearest Neighbor', knn),\n",
    "              ('Classification Tree', dt)]\n",
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging \n",
    "* Bootstrap Aggregation \n",
    "* Reduce variance of individual models in the ensamble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging classifier in sklearn \n",
    "from sklearn.ensamble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = SEED)\n",
    "dt = DecisionTreeRegressor(max_depth = 4, \n",
    "                           min_sample_leaf = 0.13,\n",
    "                           random_state = SEED) \n",
    "bc = BaggingClassifier(base_estimator = dt, n_estimators = 300, n_jobs = -1)\n",
    "\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of bag evaluation (OOB evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB evaluation in sklearn\n",
    "from sklearn.ensamble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = SEED)\n",
    "dt = DecisionTreeRegressor(max_depth = 4, \n",
    "                           min_sample_leaf = 0.16,\n",
    "                           random_state = SEED)\n",
    "bc = BaggingClassifier(base_estimator = dt, \n",
    "                       n_estimators = 300, \n",
    "                       oob_score = True, n_jobs = -1)\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "oob_accuracy = bc.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor in sklearn \n",
    "from sklearn.ensamble import RandomForestRegressor\n",
    "from sklearn.model_seelcion import trian_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = SEED)\n",
    "rf = RandomForestRegressor(n_estimators = 400, \n",
    "                           min_sample_leaf = 0.12, \n",
    "                           random_state = SEED)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "importances_rf = pd.Series(rf_feature_importances_, index = X.columns)\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "sorted_importances_rf.plot(kind = 'barh', color = 'lightgreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost \n",
    "* Stands for Adaptive Bosting\n",
    "* Achieved by changing the weights of training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classification in sklearn \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = SEED)\n",
    "dt = DecisionTreeClassifier(max_depth = 1, random_state = SEED)\n",
    "adb_clf. = AdaBoostClassifier(base_estimator = dt, n_estimators = 100)\n",
    "adb_clf.fit(X_train, y_train)\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:, 1]\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (GB)\n",
    "* Gradient Boosted Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting in sklearn \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "SEED = 1\n",
    "# 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = SEED)\n",
    "gbt = GradientBoostingRegressor(n_estimators = 300, \n",
    "                                max_depth = 1, \n",
    "                                random_state = SEED)\n",
    "gbt.fit(X_train, y_train)\n",
    "y_pred = gbt.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Boosting in sklearn \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "SEED = 1\n",
    "# 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = SEED)\n",
    "gbt = GradientBoostingRegressor(n_estimators = 300, \n",
    "                                max_depth = 1, \n",
    "                                random_state = SEED)\n",
    "sgbt = GradientBoostingRegressor(max_depth = 1, \n",
    "                                 subsample = 0.8, \n",
    "                                 max_features = 0.2,\n",
    "                                 n_estimators = 300, \n",
    "                                 random_state = SEED)\n",
    "sgbt.fit(X_train, y_train)\n",
    "sgbt_pred = sgbt.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning a CART's hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the hyperparamenters of a CART in sklearn \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "SEED = 1\n",
    "dt = DecisionTreeClassifier(random_state = SEED)\n",
    "dt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params_dt = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "    'max_features': [0.2, 0.4, 0.6, 0.8]\n",
    "}\n",
    "grid_dt = GridSearchCV(estimator = dt, \n",
    "                       pram_grid = params_dt,\n",
    "                       scoring = 'accuracy',\n",
    "                       cv = 10, \n",
    "                       n_jobs = -1)\n",
    "best_hyperparams = grid_dt.best_pramas_\n",
    "best_CV_score = grid_dt.best_score_\n",
    "best_model = grid_dt.best_estimator_\n",
    "test_acc = best_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning an RF's Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting RF hyperparamenters in sklearn \n",
    "from sklearn.ensamble import RandomForestRegressor\n",
    "SEED = 1\n",
    "rf = RandomForestRegressor(random_state = SEED)\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parms_rf = {\n",
    "    'n_estimators': [300, 400, 500],\n",
    "    'max_depth': [4, 6, 8], \n",
    "    'min_samples_leaf': [0.1, 0.2],\n",
    "    'max_features': ['log2', 'sqrt']\n",
    "}\n",
    "grid_rf = GridSearchCV(estimator = rf, \n",
    "                       pram_grid = params_rf,\n",
    "                       scoring = 'neg_mean_squared_error',\n",
    "                       cv = 3, \n",
    "                       verbose = 1,\n",
    "                       n_jobs = -1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best.hyperparams = grid_rf.best_params_\n",
    "best_model = grid_rf.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
